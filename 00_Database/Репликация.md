– это хранение копий одних и тех же данных на разных машинах, соединенных с помощью сети.

<mark style="background: #FFB86CA6;">Что дает:</mark>
- повышает надежность - в случае отказа одной из частей системы, данные можно взять из реплики
- уменьшает время получения ответа от БД - можем хранить данные географически ближе к пользователю, сокращая сетевые задержки
- повышает пропускную способность за счет горизонтального масштабирования, распределяя нагрузку по количеству машин

**Реплика** – копия БД, в которой хранятся данные. Во всех репликах в один и тот же момент времени должны быть одинаковые данные. Если данные  будут разные, появляется проблема их согласованности: 
	*при одновременном выполнении одного и того же запроса к разным репликам результаты могут оказаться различными*.
 Данная проблема является ключевой в теме репликации.

Существует несколько подходов для решения данной проблемы.

##### Репликация с ведущим узлом

![[Master-slave.png]]
`Master` используется для записи данных
`Slaves` используется для чтения данных

Обновления данных в ведомых узлах(`slaves`) могут происходить **синхронно** и **асинхронно**.
![[Pasted image 20240719182653.png]]

Репликация на ведомый узел `Slave 1` синхронна, ждем ее завершения.
Завершения асинхронной репликации на ведомый узел `Slave 2` не ждем.

- Синхронная репликация <mark style="background: #BBFABBA6;">дает гарантию</mark> того, что копия на ведомом узле согласована с мастером и актуальна.
- <mark style="background: #FFB8EBA6;">Записать данные не удастся</mark>, если синхронный slave не отвечает, поэтому делать все ведомые узлы синхронными неразумно.


Зачастую самая большая нагрузка идет на чтение из БД. В этом случае удобно создать множество ведомых узлов и распределить read нагрузку на них. Однако такой подход реально работает только при *асинхронной* репликации – если реплицировать синхронно на все slaves, то отказ одного из них сделает всю систему недоступной для операций записи, а при полностью асинхронной конфигурации, ведущий узел может записывать данные даже в случае запаздывания ответа от ведомых.

С другой стороны, сервис, который читает данные с асинхронного slave, может получать устаревшую информацию, если такой узел запаздывает. Это приводит к несогласованности БД. Кроме того, если сделать запись во все ведомые узлы асинхронной, то при сбое в мастере все не реплицированные на slaves данные потеряются. 

###### Как добавить новые ведомые узлы

1. Сделать снимок состояния БД master узла
2. Скопировать снимок на новый узел
3. Новый ведомый узел подключается к мастеру и запрашивает все изменения данных, которые произошли с момента создания снимка
4. После обновления новый slave обрабатывает поступающие изменения данных от ведущего

##### Репликация с несколькими ведущими узлами

У репликации с одним ведущим узлом есть крупный недостаток: только 1 ведущий узел, через который должны проходить все `write`-операции. Если подключиться к мастеру невозможно, то нельзя выполнить и запись в БД. В таком случае на помощь приходит репликация с несколькими ведущими узлами.

![[Pasted image 20240719182622.png]]
В конфигурации с несколькими дата-центрами данные реплицируются между различными географически распределёнными ЦОДами. Это увеличивает отказоустойчивость и улучшает время доступа, предоставляя пользователям возможность работать с ближайшим узлом.

Однако, основная проблема такой репликации – возможность возникновения конфликтов записи, которые требуют разрешения.
Например, 2 пользователя редактируют одну и ту же запись:

![[Pasted image 20240719182736.png]]

Решение – присвоить каждой операции записи уникальный id (метку даты/времени, uuid или хэш ключа и значения), после выбрать операцию с максимальным значением этого идентификатора, а остальные отбросить.

###### Топологии репликации с несколькими master

![[Pasted image 20240719171907.png]]

Проблема "кольца" и "звезды" – отказ даже одного узла способен прервать поток сообщений репликации между другими узлами, делая связь между ними невозможной. Отказоустойчивость "каждый с каждым" выше, поскольку позволяет сообщениям перемещаться различными путями, обходя отказавшие узлы.
С другой стороны, в "каждый с каждым" одни сетевые ссылки могут быть быстрее других, в результате чего одни сообщения репликации могут обгонять другие.

##### Репликации без ведущего узла 

Каждый узел может обрабатывать как запросы на запись, так и запросы на чтение. Это повышает доступность и отказоустойчивость системы.

![[Pasted image 20240719182841.png]]

- Когда узел получает запрос на запись, он должен убедиться, что запись может быть выполнена на достаточном количестве узлов, чтобы избежать конфликтов. Это достигается с помощью механизма кворума. Например, для успешной записи может потребоваться, чтобы запись была подтверждена как минимум на `w` узлах, где w — это заранее определенное количество узлов, необходимых для подтверждения записи.
- Для операций чтения может потребоваться подтверждение от `r` узлов, к которым параллельно отправляются запросы. Если запрашиваемые данные находятся на нескольких узлах, система может выбрать, откуда читать, основываясь на текущем состоянии данных и доступности узлов. Если узлы не синхронизированы, это может привести к чтению устаревших данных, поэтому важно правильно настроить кворумы для чтения и записи, чтобы минимизировать такие ситуации.


![[Pasted image 20240719220945.png]]